# -*- coding: utf-8 -*-
"""88%AccuracyProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ruBrnx3fLDuHBtNC49lIose2i4JLGVZv
"""

!pip install pennylane

import pennylane as qml
from pennylane import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report

data = pd.read_csv('file3.csv')
X = data.drop('diagnosis', axis=1).values
y = data['diagnosis'].values

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Quantum Device Setup
n_qubits = X_train.shape[1]
dev = qml.device('default.qubit', wires=n_qubits)

@qml.qnode(dev)
def quantum_feature_extractor(inputs, weights):
    for i in range(len(inputs)):
        qml.RY(inputs[i], wires=i)
        qml.RZ(inputs[i], wires=i)
    qml.StronglyEntanglingLayers(weights=weights, wires=range(n_qubits))
    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]

# Function to Extract Features
def extract_quantum_features(X, weights):
    return np.array(qml.execute([quantum_feature_extractor], dev, [weights], interface='autograd', gradient_fn=quantum_feature_extractor.gradient)([X]))

n_layers = 2
weights_shape = qml.StronglyEntanglingLayers.shape(n_layers=n_layers, n_wires=n_qubits)
weights = np.random.uniform(-0.1, 0.1, size=weights_shape, requires_grad=True)

X_train_q = extract_quantum_features(X_train_scaled, weights)
X_test_q = extract_quantum_features(X_test_scaled, weights)

clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train_q, y_train)

data.head()

data.stage.value_counts()

df = pd.read_csv('/content/Normalized_Dataset.csv')

from sklearn.model_selection import train_test_split

df.drop(columns=['benign_sample_diagnosis'], inplace=True)

df.diagnosis.value_counts()

X = df.drop('diagnosis', axis=1).values

from sklearn.preprocessing import LabelEncoder
y = df['diagnosis'].values
encoder = LabelEncoder()
y = encoder.fit_transform(y)

print(y)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

print(y_train)

X_train.shape[1]

num_qubits = 10
device = qml.device('default.qubit', wires=num_qubits)

@qml.qnode(device)
def quantum_feature_extractor(inputs, weights):
    for i in range(len(inputs)):
        qml.RY(inputs[i], wires=i)
        qml.RZ(inputs[i], wires=i)
    qml.StronglyEntanglingLayers(weights=weights, wires=range(n_qubits))
    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]

# Function to Extract Features
def extract_quantum_features(X, weights):
    return np.array([quantum_feature_extractor(x, weights) for x in X], dtype=float)

# Generate Quantum Features
n_layers = 4
weights_shape = qml.StronglyEntanglingLayers.shape(n_layers=n_layers, n_wires=n_qubits)
weights = np.random.uniform(-0.1, 0.1, size=weights_shape)

X_train_q = extract_quantum_features(X_train_scaled, weights)
X_test_q = extract_quantum_features(X_test_scaled, weights)

df.diagnosis.value_counts()

print(y_test)

svm_clf = SVC(kernel='rbf')
svm_clf.fit(X_train_q, y_train)
y_pred_svm = svm_clf.predict(X_test_q)
print("SVM Classifier on Quantum Features:")
print(classification_report(y_test, y_pred_svm))

clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train_q, y_train)

y_pred = clf.predict(X_test_q)
print("Random Forest Classifier on Quantum Features:")
print(classification_report(y_test, y_pred))

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
print(accuracy_score(y_test, y_pred))
print(precision_score(y_test, y_pred, average='weighted')) # Change is here
print(recall_score(y_test, y_pred, average='weighted')) # Change is here
print(f1_score(y_test, y_pred, average='weighted')) # Change is here
print(roc_auc_score(y_test, y_pred, multi_class='ovr', average='weighted')) # Change is here
print(confusion_matrix(y_test, y_pred))



!pip install lazypredict

from lazypredict.Supervised import LazyClassifier

clf = LazyClassifier()
models, predictions = clf.fit(X_train_q, X_test_q, y_train, y_test)

# Print Model Performance
print(models)

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from lazypredict.Supervised import LazyClassifier
from xgboost import XGBClassifier

pca = PCA(n_components=8)  # Reduce dimensionality
X_train_q = pca.fit_transform(X_train_q)
X_test_q = pca.transform(X_test_q)

# Use LazyPredict for Model Comparison
clf = LazyClassifier()
models, predictions = clf.fit(X_train_q, X_test_q, y_train, y_test)
print(models)

